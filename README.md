# Mitigating Multi-Sequence 3D Prostate MRI Data Scarcity through Domain Adaptation using Locally-Trained Latent Diffusion Models for Prostate Cancer Detection

**Authors:** Emerson P. Grabke, Babak Taati*, Masoom A. Haider*
**arXiv:** [arXiv:2507.06384](https://arxiv.org/abs/2507.06384) 
**PDF:** [Download PDF](https://arxiv.org/pdf/2507.06384.pdf)

*BT and MAH are co-senior authors on this work

Manuscript has been submitted to IEEE for possible publication

## Abstract

> Objective: Latent diffusion models (LDMs) could mitigate data scarcity challenges affecting machine learning development for medical image interpretation. The recent CCELLA LDM improved prostate cancer detection performance using synthetic MRI for classifier training but was limited to the axial T2-weighted (AxT2) sequence, did not investigate inter-institutional domain shift, and prioritized radiology over histopathology outcomes. We propose CCELLA++ to address these limitations and improve clinical utility. Methods: CCELLA++ expands CCELLA for simultaneous biparametric prostate MRI (bpMRI) generation, including the AxT2, high b-value diffusion series (HighB) and apparent diffusion coefficient map (ADC). Domain adaptation was investigated by pretraining classifiers on real or LDM-generated synthetic data from an internal institution, followed with fine-tuning on progressively smaller fractions of an out-of-distribution, external dataset. Results: CCELLA++ improved 3D FID for HighB and ADC but not AxT2 (0.013, 0.012, 0.063 respectively) sequences compared to CCELLA (0.060). Classifier pretraining with CCELLA++ bpMRI outperformed real bpMRI in AP and AUC for all domain adaptation scenarios. CCELLA++ pretraining achieved highest classifier performance below 50% (n=665) external dataset volume. Conclusion: Synthetic bpMRI generated by our method can improve downstream classifier generalization and performance beyond real bpMRI or CCELLA-generated AxT2-only images. Future work should seek to quantify medical image sample quality, balance multi-sequence LDM training, and condition the LDM with additional information. Significance: The proposed CCELLA++ LDM can generate synthetic bpMRI that outperforms real data for domain adaptation with a limited target institution dataset, advancing machine learning development for medical imaging.

## Code
This repository contains the scripts needed to train the CCELLA and CCELLA++ LDMs in this work. All scripts are expected to be run from the ccellapp folder, with examples for multi-gpu use where available.

### 1. Starting Requirements
All data should already be resampled to the desired (fixed) image size. The JSON files at ./datasets/train.json and ./datasets/test.json should contain non-overlapping entries for each data sample, with the "text" field of each report containing the plaintext radiology report for that exam (or "" if empty in which case "text_isnull" should be set to 1). The PI-RADS vector should be onehot and set to `[neg,pos]` (i.e. `[1,0]` if PI-RADS in `[1,2]`, else `[0,1]`).

There should additionally be an Excel spreadsheet (.xlsx) with the following columns:
  - "folder" : patient ID and subdirectory
  - "PosPIRADS" : Positive or negative (or empty) based on the case PI-RADS grading
  - "PosISUP" : Positive or negative (or empty) based on the case ISUP grading
  - "Label": Positive or negative based on the PIRADS and/or ISUP labels

Note that one of PosPIRADS or PosISUP needs to be nonempty for every row.

### 2. Data Preprocessing
Two scripts should be run in succession to generate the image and text embeddings for use in the LDM training pipeline. Example run:
```
export NUM_GPUS_PER_NODE=2
torchrun \
  --nproc_per_node=${NUM_GPUS_PER_NODE} \
  --nnodes=1 \
  --master_addr=localhost --master_port=1234 \
  -m scripts.data_processing.diff_model_create_training_data --model_def ./configs/CCELLA_def.json --model_config ./configs/CCELLA_config.json --env_config ./configs/CCELLA_env.json --num_gpus ${NUM_GPUS_PER_NODE}
python -m scripts.data_processing.gen_json_maisi_merged
```

### 3. LDM Training
LDMs can be trained using the following command:
```
export NUM_GPUS_PER_NODE=2
CUDA_VISIBLE_DEVICES=0,1 torchrun \
    --nproc_per_node=${NUM_GPUS_PER_NODE} \
    --nnodes=1 \
    --master_addr=localhost --master_port=1234 \
    -m scripts.diff_model_train_all --model_def ./configs/CCELLApp_def.json --model_config ./configs/CCELLApp_config.json --env_config ./configs/CCELLApp_env.json --num_gpus ${NUM_GPUS_PER_NODE}
```

JSON files for each of the model definition, model config, and environment config have been provided in ./configs/

### 4. Synthetic Data Generation
To generate synthetic images (e.g. for LDM evaluation and use in a downstream domain adaptation task):
```
CUDA_VISIBLE_DEVICES=0,1 torchrun \
  --nproc_per_node=4 \
  --nnodes=1 \
  --master_addr=localhost --master_port=1234 \
  -m scripts.synthetic_datagen --model_def ./configs/CCELLApp_def.json --model_config ./configs/CCELLApp_config.json --env_config ./configs/CCELLApp_env.json --num_gpus 2
```

Alongside generating the images, the scripts will generate multiple Excel sheets per GPU and per each of train and test splits, in the form "{output_dir}/synthsheet_{local_rank}_{train/test}.xlsx". These need to be merged manually into a single spreadsheet, combined with Institute 2 cases, and with a new column "ISUP_real" added containing the histopathology values for all cases. Merged spreadsheet should be saved to "{output_dir}/synthsheet_tfproxy.xlsx" for CCELLA, "{output_dir}/synthsheet_tfproxy_multi.xlsx" for CCELLA++

### 5. LDM Evaluation
LDM evaluation can occur after synthetic data generation with:
```
python -m scripts.ldm_metrics --multi
```

Remove the flag --multi for CCELLA evaluation.

### 6. Domain Adaptation Studies
Downstream EfficientNet classifiers and their domain adaptations can be trained and evaluated with the following commands (note: -u denotes Institute1, -p denotes Institute2 for one-letter abbreviation):

When training from scratch on Institute1 real AxT2-only followed by fine-tune at multiple Institute2 fractions:
```
export DEVICES=0
CUDA_VISIBLE_DEVICES=${DEVICES} python -m scripts.tfproxy -u -r -c 1
CUDA_VISIBLE_DEVICES=${DEVICES} python -m scripts.tfproxy -u -r -p -f -c 1
CUDA_VISIBLE_DEVICES=${DEVICES} python -m scripts.tfproxy_multifrac -f -r -c 1
CUDA_VISIBLE_DEVICES=${DEVICES} python -m scripts.tfproxy_metrics_multifrac --exp_name tfproxy_enb0_ur --count_start 1 --count_end 1
```

For Institute1 synthetic AxT2-only from scratch followed by Institute2 fine-tune:
```
CUDA_VISIBLE_DEVICES=${DEVICES} python -m scripts.tfproxy -u -s -c 1
CUDA_VISIBLE_DEVICES=${DEVICES} python -m scripts.tfproxy -u -s -p -f -c 1
CUDA_VISIBLE_DEVICES=${DEVICES} python -m scripts.tfproxy_multifrac -f -s -c 1
CUDA_VISIBLE_DEVICES=${DEVICES} python -m scripts.tfproxy_metrics_multifrac --exp_name tfproxy_enb0_us_l1 --count_start 1 --count_end 1
```

For Institute2 AxT2-only training from scratch:
```
CUDA_VISIBLE_DEVICES=${DEVICES} python -m scripts.tfproxy -p -c 1
CUDA_VISIBLE_DEVICES=${DEVICES} python -m scripts.tfproxy_multifrac -c 1
CUDA_VISIBLE_DEVICES=${DEVICES} python -m scripts.tfproxy_metrics_multifrac --exp_name tfproxy_enb0_p --count_start 1 --count_end 1
```

For Institute1 real bpMRI from scratch followed by Institute2 fine-tune:
```
CUDA_VISIBLE_DEVICES=${DEVICES} python -m scripts.tfproxy -i multi -u -r -c 1
CUDA_VISIBLE_DEVICES=${DEVICES} python -m scripts.tfproxy -i multi -u -r -p -f -c 1
CUDA_VISIBLE_DEVICES=${DEVICES} python -m scripts.tfproxy_multifrac -i multi -f -r -c 1
CUDA_VISIBLE_DEVICES=${DEVICES} python -m scripts.tfproxy_metrics_multifrac -i multi --exp_name tfproxy_enb0_ur --count_start 1 --count_end 1
```

For Institute1 synthetic bpMRI from scratch followed by Institute2 fine-tune:
```
CUDA_VISIBLE_DEVICES=${DEVICES} python -m scripts.tfproxy -i multi -u -s -c 1
CUDA_VISIBLE_DEVICES=${DEVICES} python -m scripts.tfproxy -i multi -u -s -p -f -c 1
CUDA_VISIBLE_DEVICES=${DEVICES} python -m scripts.tfproxy_multifrac -i multi -f -s -c 1
CUDA_VISIBLE_DEVICES=${DEVICES} python -m scripts.tfproxy_metrics_multifrac -i multi --exp_name tfproxy_enb0_us --count_start 1 --count_end 1
```

For Institute2 bpMRI training from scratch:
```
CUDA_VISIBLE_DEVICES=${DEVICES} python -m scripts.tfproxy -p -i multi -c 1
CUDA_VISIBLE_DEVICES=${DEVICES} python -m scripts.tfproxy_multifrac -i multi -c 1
CUDA_VISIBLE_DEVICES=${DEVICES} python -m scripts.tfproxy_metrics_multifrac -i multi --exp_name tfproxy_enb0_p --count_start 1 --count_end 1
```

The -c flag can be adjusted to train multiple different (i.e. different-seed) models on the same dataset. Metrics can be aggregated by setting --count_end to the last -c value used (inclusive) to include each of those models in the metric calculation.

## Acknowledgements
The code in this repository has built upon publicly released code from [CCELLA](https://github.com/grabkeem/CCELLA) which built atop code from [MONAI](https://github.com/Project-MONAI/MONAI/), [MAISI](https://github.com/Project-MONAI/tutorials/tree/main/generation/maisi), and [ELLA](https://github.com/TencentQQGYLab/ELLA)